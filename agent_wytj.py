import streamlit as st
import requests
import uuid
import dashscope
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory
from langchain.tools import tool
from langchain_community.llms.tongyi import Tongyi
import os
from langchain.agents import initialize_agent, AgentType, Tool
from langchain_core.prompts import PromptTemplate
from openai import OpenAI
from langchain_openai import ChatOpenAI
from flask import Flask, request, jsonify
from streamlit.components.v1 import html
import base64
import threading
import time

# é¡µé¢é…ç½®
st.set_page_config(page_title="ç½‘é¡µæ¨è", page_icon="ğŸŒ")

# è‡ªå®šä¹‰ CSS æ ·å¼
custom_css = """
<style>
    h1 {
        font-size: 2.5rem;
        color: #333;
        text-align: center;
    }

    h2 {
        font-size: 1.5rem;
        color: #666;
        text-align: center;
        margin-bottom: 20px;
    }

    .result-block {
        border: 1px solid #ddd;
        padding: 15px;
        margin-bottom: 20px;
        border-radius: 8px;
        background-color: #f9f9f9;
    }

    .result-block h3 {
        font-size: 1.2rem;
        color: #4a4a4a;
        margin-bottom: 10px;
    }

    .result-block p {
        font-size: 1rem;
        color: #555;
        margin-bottom: 5px;
    }

    .result-block a {
        display: inline-block;
        margin-top: 10px;
        padding: 8px 12px;
        background-color: #007BFF;
        color: white;
        text-decoration: none;
        border-radius: 5px;
    }
</style>
"""
st.markdown(custom_css, unsafe_allow_html=True)

# é¡µé¢æ ‡é¢˜å’Œå°æ ‡é¢˜
st.title("ç½‘é¡µæ¨è")
st.markdown('<h2 style="font-family: Arial;">è§‚ä¸–é—´ä¸‡è±¡ï¼Œæ­¤å¿ƒå½“å®</h2>', unsafe_allow_html=True)

# è®¾ç½® API å¯†é’¥
os.environ["DASHSCOPE_API_KEY"] = os.getenv("DASHSCOPE_API_KEY", "sk-38a6f574d6c6483eae5c32998a16822a")
os.environ["DASHSCOPE_API_BASE"] = os.getenv("DASHSCOPE_API_BASE", "https://dashscope.aliyuncs.com/compatible-mode/v1")



# åˆ›å»ºç½‘ç»œæœç´¢å·¥å…·
@tool
def bocha_websearch_tool(query: str, count: int = 20) -> str:
    """
    ä½¿ç”¨Bocha Web Search API ç½‘é¡µæœç´¢
    """
    url = 'https://api.bochaai.com/v1/web-search'
    headers = {
        'Authorization': f'Bearer sk-6012a020f72d4c26ae5ad415300c94f9',
        'Content-Type': 'application/json'
    }
    data = {
        "query": query,
        "freshness": "noLimit",
        "summary": True,
        "count": count
    }

    response = requests.post(url, headers=headers, json=data)
    if response.status_code == 200:
        try:
            json_response = response.json()
            if json_response["code"] == 200 and json_response.get("data"):
                webpages = json_response["data"]["webPages"]["value"]
                if not webpages:
                    return "æœªæ‰¾åˆ°ç›¸å…³ç»“æœ."
                formatted_results = ""
                for idx, page in enumerate(webpages, start=1):
                    formatted_results += (
                        f"å¼•ç”¨ï¼š{idx}\n"
                        f"æ ‡é¢˜ï¼š{page['name']}\n"
                        f"URL: {page['url']}\n"
                        f"æ‘˜è¦ï¼š{page['summary']}\n"
                        f"ç½‘ç«™åç§°ï¼š{page['siteName']}\n"
                        f"ç½‘ç«™å›¾æ ‡ï¼š{page['siteIcon']}\n"
                        f"å‘å¸ƒæ—¶é—´ï¼š{page['dateLastCrawled']}\n\n"
                    )
                return formatted_results.strip()
            else:
                return f"æœç´¢å¤±è´¥ï¼ŒåŸå› ï¼š{json_response.get('message', 'æœªçŸ¥é”™è¯¯')}"
        except Exception as e:
            return f"å¤„ç†æœç´¢ç»“æœå¤±è´¥ï¼ŒåŸå› æ˜¯ï¼š{str(e)}\nåŸå§‹å“åº”ï¼š{response.text}"
    else:
        return f"æœç´¢APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}, é”™è¯¯ä¿¡æ¯ï¼š{response.text}"


memory = ConversationBufferMemory(memory_key="chat_history",return_messages=True)

llm = ChatOpenAI(
    model="qwen-max",
    temperature=0.8,
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

bocha_tool = Tool(
    name="Bocha Web Search",
    func=bocha_websearch_tool,
    description="ä½¿ç”¨Bocha Web Search APIè¿›è¡Œæœç´¢äº’è”ç½‘ç½‘é¡µï¼Œè¾“å…¥åº”ä¸ºæœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œè¾“å‡ºå°†è¿”å›æœç´¢ç»“æœçš„è¯¦ç»†ä¿¡æ¯ã€‚åŒ…æ‹¬ç½‘é¡µæ ‡é¢˜ã€ç½‘é¡µURL",
)



#æœç´¢å·¥å…·æç¤ºè¯
agent_prompt = """
å®ç”¨å¿ƒç†ç½‘é¡µ
å®ç”¨å¿ƒç†ç½‘ç«™
å¿ƒç†å°æ¸¸æˆ
å¿ƒç†äº’åŠ¨
å¿ƒç†çŸ¥è¯†å°ç§‘æ™®
å¿ƒç†å°è´´å£«
 å¿ƒç†æˆé•¿æ•…äº‹
 å¿ƒç†è‡ªæ„ˆçˆ†æ¬¾
HTTPçŠ¶æ€ç 200 ç›´æ¥è®¿é—®
æ— CookieéªŒè¯ å¿ƒç† ç›´é“¾
æ— Refereré™åˆ¶  HTTPSé“¾æ¥
æˆé•¿
æ²»æ„ˆç³»

è¯»å–åœ¨bocha_toolè¿”å›ç»“æœä¸­å¯ç”¨çš„ç½‘å€é“¾æ¥ï¼Œå¹¶è¿”å›
"""


agent = initialize_agent(
    tools=[bocha_tool],
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    memory=memory,
    verbose=True,
    agent_kwargs={"agent_prompt": agent_prompt, 'memory': memory}
)




#å¤§è¯­è¨€æ¨¡å‹æç¤ºè¯
prompt_template_with_search_results = """
{previous_conversation}

æœ€æ–°çš„æœç´¢ç»“æœå¦‚ä¸‹ï¼š
{search_results}

è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯æ¨èè¿‘æœŸå¯ç›´æ¥è®¿é—®çš„å®ç”¨å¿ƒç†å¥åº·ç½‘ç«™,è¦æ±‚ï¼š
1. å¿…é¡»æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼š
   - ç›´è¿é“¾æ¥ï¼ˆéé¦–é¡µè·³è½¬ï¼‰
   - å†…å®¹å«å¯æ“ä½œå¿ƒç†æŠ€å·§æˆ–æƒå¨èƒŒä¹¦
2. è¾“å‡ºæ ¼å¼ä¸¥æ ¼éµå¾ªï¼š
   - å›½å†…ç›´è¿é“¾æ¥ï¼ˆHTTPSåè®®ï¼Œå·²éªŒè¯å¯è®¿é—®ï¼Œå¹¶ä¸”åº”è¯¥åœ¨bocha_toolçš„æœç´¢è¿”å›ç»“æœï¼‰
   - ç½‘ç«™æ ¸å¿ƒæ ‡ç­¾ï¼ˆå¦‚ï¼šæ­£å¿µç»ƒä¹ /è®¤çŸ¥é‡æ„ï¼‰
3. æ’é™¤ä»¥ä¸‹ç±»å‹ï¼š
   - éœ€è¦å¾®ä¿¡æ‰«ç ç™»å½•çš„å†…å®¹
   - æ˜¾ç¤º"è¯•è¯»ç»“æŸ"çš„ç‰‡æ®µ
   - å¼ºåˆ¶è·³è½¬åˆ°Appä¸‹è½½çš„é“¾æ¥
   - æåŠæˆ–æš—ç¤ºå¿ƒç†ç–¾ç—…çš„ç½‘ç«™
   - æ’é™¤ç ”ç©¶æ‰€ã€ç ”ç©¶é™¢ã€åŒ»é™¢å®˜ç½‘ç­‰ç§‘ç ”å‘å¹³å°
"""

final_prompt = PromptTemplate(
    input_variables=["previous_conversation", "search_results"],
    template=prompt_template_with_search_results
)



llm_chat = ChatOpenAI(
    model="qwen-max-latest",
    temperature=0.8,
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

chain = LLMChain(llm=llm_chat, prompt=final_prompt)





#ç”¨æˆ·æé—®ï¼ˆåŠŸèƒ½ç›¸å…³ï¼‰
user_question = "æˆ‘æƒ³å¾—åˆ°ä¸€äº›æœ‰åŠ©äºå¿ƒç†å¥åº·çš„ç½‘é¡µæ¨è,è¯·ç»™æˆ‘å¯ç”¨çš„ç½‘ç»œé“¾æ¥"


response = agent.run(user_question)

# å‡†å¤‡è¾“å…¥ç»™ Final Prompt çš„æ•°æ®
inputs = {
    "previous_conversation": "\n".join([str(message) for message in memory.load_memory_variables({})["chat_history"]]),
    "search_results": response
}

final_response=chain.run(inputs)


st.write(final_response)
